# Quiz

## Question 1
Interacting with Large Language Models (LLMs) differs from traditional machine learning models. Working with LLMs involves natural language input, known as a _____, resulting in output from the Large Language Model, known as the ______ .

Choose the answer that correctly fills in the blanks.

1 point

- [ ] prompt, fine-tuned LLM
- [x] prompt, completion
- [ ] prediction request, prediction response
- [ ] tunable request, completion

## Question 2
Large Language Models (LLMs) are capable of performing multiple tasks supporting a variety of use cases. Which of the following tasks supports the use case of converting code comments into executable code?

1 point

- [x] Invoke actions from text
- [ ] Information Retrieval
- [ ] Translation
- [ ] Text summarization

## Question 3
What is the self-attention that powers the transformer architecture?

1 point

- [ ] A technique used to improve the generalization capabilities of a model by training it on diverse datasets.
- [ ] The ability of the transformer to analyze its own performance and make adjustments accordingly.
- [x] A mechanism that allows a model to focus on different parts of the input sequence during computation.
- [ ] A measure of how well a model can understand and generate human-like language.

## Question 4
Which of the following stages are part of the generative AI model lifecycle mentioned in the course? (Select all that apply)

1 point

- [x] Manipulating the model to align with specific project needs.
- [x] Selecting a candidate model and potentially pre-training a custom model.
- [ ] Performing regularization
- [x] Defining the problem and identifying relevant datasets.
- [x] Deploying the model into the infrastructure and integrating it with the application.

## Question 5
"RNNs are better than Transformers for generative AI Tasks."

Is this true or false?

1 point

- [ ] True
- [x] False

## Question 6
Which transformer-based model architecture has the objective of guessing a masked token based on the previous sequence of tokens by building bidirectional representations of the input sequence.

1 point

- [ ] Sequence-to-sequence
- [x] Autoencoder
- [ ] Autoregressive

## Question 7
Which transformer-based model architecture is well-suited to the task of text translation?

1 point

- [ ] Autoregressive
- [x] Sequence-to-sequence
- [ ] Autoencoder

## Question 8
Do we always need to increase the model size to improve its performance?

1 point

- [ ] True
- [x] False

## Question 9
Scaling laws for pre-training large language models consider several aspects to maximize performance of a model within a set of constraints and available scaling choices. Select all alternatives that should be considered for scaling when performing model pre-training?

1 point

- [x] Model size: Number of parameters
- [ ] Batch size: Number of samples per iteration
- [x] Compute budget: Compute constraints
- [x] Dataset size: Number of tokens

## Question 10
"You can combine data parallelism with model parallelism to train LLMs."

Is this true or false?

1 point

- [x] True
- [ ] False
